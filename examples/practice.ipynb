{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MIT License\\n\\nCopyright (c) 2022 AkulS1008\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def file_read(fpath = r\"LICENSE\"):\n",
    "    if not os.path.exists(fpath):\n",
    "        print(\"File not found\")\n",
    "        return None\n",
    "    if not os.path.isfile(fpath):\n",
    "        print(\"Not a file\")\n",
    "        return None\n",
    "    with open(fpath) as file:\n",
    "        c = file.read()\n",
    "        return c\n",
    "file_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #used to collect the page from the web\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://python-poetry.org/docs/basic-usage/\"\n",
    "ua = UserAgent()\n",
    "headers = {'User-Agent' : ua.random}\n",
    "page = requests.get(url, headers = headers, cookies = {\"session-id\" : \"\", \"session-id-time\" : \"\", \"session-token\" : \"\"})\n",
    "print(page)\n",
    "if page.status_code == 404:\n",
    "    print(\"Page not found\")\n",
    "elif page.status_code == 503:\n",
    "    print(\"Page unavailable\")\n",
    "elif page.status_code == 200:\n",
    "    print(\"Page found\")\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font('helvetica', size =12)\n",
    "pdf.cell(txt=\"Sample text\")\n",
    "pdf.output(\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import re\n",
    "\n",
    "def __validate_url__(url):\n",
    "    regex = re.compile(r'^(?:http)s?://', re.IGNORECASE)\n",
    "    return re.match(regex, url) is not None\n",
    "\n",
    "def __clean_url__(url):\n",
    "    if '?' in url:\n",
    "        url = url.split('?')[0]\n",
    "    return url\n",
    "\n",
    "def get_webpage_data(url, headers = None, cookies = None) -> BeautifulSoup:\n",
    "    url = __clean_url__(url)\n",
    "    if not __validate_url__(url):\n",
    "        print(\"Invalid url\")\n",
    "        return None\n",
    "    if headers is None:\n",
    "        ua = UserAgent()\n",
    "        headers = {'User-Agent' : ua.random}\n",
    "    if cookies is None:\n",
    "        cookies = {\"session-id\" : \"\", \"session-id-time\" : \"\", \"session-token\" : \"\"}\n",
    "    try:\n",
    "        page = requests.get(url, headers = headers, cookies = cookies)\n",
    "        if page.status_code == 404:\n",
    "            print(\"Page not found\")\n",
    "            return None\n",
    "        elif page.status_code == 503:\n",
    "            print(\"Page unavailable\")\n",
    "            return None\n",
    "        elif page.status_code == 200:\n",
    "            print(\"Page found\")\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            return soup\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "def extract_one(soup : BeautifulSoup, **selectors) -> dict:\n",
    "    if not isinstance(soup, BeautifulSoup):\n",
    "        print(\"Not a BeautifulSoup object\")\n",
    "        return None\n",
    "    data = {}\n",
    "    try:\n",
    "        for key,info in selectors.items():\n",
    "            tag = info.get('tag', 'div')\n",
    "            attrs = info.get('attrs', None) #defaults as 2nd param\n",
    "            output = info.get('output', 'text')\n",
    "            if output == 'text':\n",
    "                data[key] = soup.find(tag, attrs = attrs).text.strip()\n",
    "            elif output == 'href':\n",
    "                data[key] = soup.find(tag, attrs = attrs).attrs.get('href') \n",
    "            elif output == 'src': #for images\n",
    "                data[key] = soup.find(tag, attrs = attrs).attrs.get('src')     \n",
    "            else:\n",
    "                print('Not suitable output')\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(\"Could not extract data\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'Hurricane Leslie (2018)'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_one(get_webpage_data(\"https://en.wikipedia.org/wiki/Hurricane_Leslie_(2018)\"), title = {'tag' : 'h1', 'attrs' : {'id' : 'firstHeading'}, 'output' : 'text'})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12c474f0c101339e553446c59ed724afd66b3d962f47665a24b144af75695b9b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
